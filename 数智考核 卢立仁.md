# 数智考核

## sofxmax

### 原理及说明

**原理：**

$$softmax(z_i)=\frac{e^{z_i}}{\sum\limits_{c=1}^C e^{z_c}}$$，可以将多分类的输出值范围转换到$$[0,1]$$中。

在多分类任务中，将样本的特征输入到以$$softmax$$函数为激活函数的神经网络（以只有一层输出层为例），得到由该样本在各类别的概率所形成的向量，选取概率最高的值为预测结果。本次任务是通过不断迭代更新网络的权值及偏置，从而提高分类器分类正确的概率。

![image-20220325235623363](http://mari0.oss-cn-guangzhou.aliyuncs.com/img/image-20220325235623363.png)

**公式推导：**

数据集：以数智的10分类数据集为例。

样本：设每个样本有$$n$$个特征，整个数据集共有$$M$$个样本。

对单个样本：$$x^{(1)}=\begin{pmatrix}x^{(1)}_0 \\ x^{(1)}_1\\ \vdots \\ x^{(1)}_n\end{pmatrix}$$,$$y^{(1)}=\begin{pmatrix} y^{(1)}_0 \\ y^{(1)}_1 \\ \vdots \\ y^{(1)}_9 \end{pmatrix}$$（按照one-hot编码）,$$w^T=\begin{pmatrix} w_0 \\ w_1\\ \vdots \\ w_n \end{pmatrix}^T=(w_0,w_1\dots,w_n)$$（其中一个神经元的$$w$$）

对多个样本：$$X=\begin{pmatrix} |&|& &|\\ x^{(1)}&x^{(2)}&\dots&x^{(M)}\\|&|&&| \end{pmatrix}$$,$$W=\begin{pmatrix} -w^T_0-\\-w^T_1-\\\vdots\\-w^T_9- \end{pmatrix}$$,$$Y=\begin{pmatrix} |&|& &|\\ y^{(1)}&y^{(2)}&\dots&y^{(M)}\\|&|&&| \end{pmatrix}$$,$$b=\begin{pmatrix} b_0\\b_1\\ \vdots\\b_9 \end{pmatrix}$$

$$z^{(1)}=Wx^{(1)}+b=\begin{pmatrix} z^{(1)}_0\\\vdots\\z^{(1)}_9 \end{pmatrix}$$,$$Z=WX+b=\begin{pmatrix} |&\dots&| \\z^{(1)}&\dots&z^{(M)}\\|&\dots&| \end{pmatrix}$$

$$\because softmax(z^{(m)}_i)=\frac{e^{z^{(m)}_i}}{\sum\limits_{c=1}^C e^{z^{(m)}_c}}=p^{(m)}_i $$

$$\therefore softmax(z^{(1)})=p^{(1)}=\begin{pmatrix} p^{(1)}_0\\\vdots\\p^{(1)}_9 \end{pmatrix} $$

$$\therefore softmax(Z)=P=\begin{pmatrix} p^{(1)}_0&\dots&p^{(M)}_0\\\vdots&\dots&\vdots\\p^{(1)}_9 &\dots&p^{(M)}_9 \end{pmatrix} $$

使用交叉熵作为损失函数：

$$L(p^{(1)})=-\sum\limits_{c=1}^C y^{(1)}_c log(p^{(1)}_c)=-y^{(1)^T}log(p^{(1)})$$

对于多个样本:

$$L(P)=-\frac{1}{M}\sum\limits_{m=1}^M\sum\limits_{c=1}^C y^{(m)}_c log(p^{(m)}_c)=-\frac{1}{M}\sum\limits_{m=1}^M y^{(m)^T}log(p^{(m)})=-\frac{1}{M}\sum\ Y\cdot log(P)$$（最后用的是点乘）

对于经过one-hot编码的标签：

$$L(p^{(1)})=-log(p^{(1)}_{true})$$

对于多个样本：

$$L(P)=-\frac{1}{M}\sum\limits_{m=1}^Mlog(p^{(m)}_{true})$$

对于单个样本，下面先对$$z^{(1)}_i$$求偏导：
$$
\begin{aligned}
&\therefore \frac{\partial L(p^{(1)})}{\partial z^{(1)}_i}\\&=-\sum\limits_{c=1}^C y^{(1)}_c \frac{log(p^{(1)}_c)}{\partial z^{(1)}_i}
\\&=-\sum\limits_{c=1}^C y^{(1)}_c \frac{log(p^{(1)}_c)}{\partial p^{(1)}_c} \frac{\partial p^{(1)}_c}{\partial z^{(1)}_i}
\\&=-\sum\limits_{c=1}^C y^{(1)}_c\frac{1}{p^{(1)}_c}\frac{\partial softmax(z^{(1)}_c)}{\partial z^{(1)}_i}
\\又&\because 当c\neq i时，
\\&\frac{\partial softmax(z^{(1)}_c)}{\partial z^{(1)}_i}
\\&=\frac{0-e^{z^{(1)}_c}(\sum\limits_{c=1}^C e^{z^{(1)}_c})^{'}}{(\sum\limits_{c=1}^C e^{z^{(1)}_c})^2}
\\&=\frac{-e^{(1)}_c e^{(1)}_i}{(\sum\limits_{c=1}^C e^{z^{(1)}_c})^2}
\\&=-p^{(1)}_c p^{(1)}_i
\\&同理可得，当c=i时
\\&\frac{\partial softmax(z^{(1)}_c)}{\partial z^{(1)}_i}=p^{(1)}_c (1-p^{(1)}_i)
\\ \therefore& \frac{\partial softmax(z^{(1)}_c)}{\partial z^{(1)}_i}=\begin{equation}
\left\{
\begin{aligned}
&p^{(m)}_c (1-p^{(m)}_i)&,c=i \\
&-p^{(m)}p^{(m)}_i&,c\neq i
\end{aligned}
\right.
\end{equation}
\\ \therefore &\frac{\partial L(p^{(1)})}{\partial z^{(1)}_i}
\\&=-y^{(1)}_i (1-p^{(1)}_i)-\sum\limits_{c\neq i}-y^{(1)}_c p^{(1)}_i
\\&=-y^{(1)}_i+y^{(1)}_ip^{(1)}_i+\sum\limits_{c\neq i}y^{(1)}_c p^{(1)}_i
\\&=-y^{(1)}_i+p^{(1)}_i(y^{(1)}_i+\sum\limits_{c\neq i} y^{(1)}_c)
\\&根据one-hot编码
\\&\frac{\partial L(p^{(1)})}{\partial z^{(1)}_i}=p^{(1)}_i - y^{(1)}_i
\end{aligned}
$$
$$又\because w^T=\begin{pmatrix} w_0 \\ w_1\\ \vdots \\ w_n \end{pmatrix}^T=(w_0,w_1\dots,w_n) ,W=\begin{pmatrix} -w^T_0-\\-w^T_1-\\\vdots\\-w^T_9- \end{pmatrix}$$,记$$w^T_{ij}$$为$$w^T_i$$中的第$$j$$个元素.

$$\therefore \frac{\partial z^{(1)}_i}{\partial w^T_{ij}}=x^{(1)}_j$$

$$\therefore \frac{\partial L(p^{(1)})}{\partial w^T_{ij}}=(p^{(1)}_i-y^{(1)}_i)x^{(1)}_j$$

$$\therefore \frac{\partial L(p^{(1)})}{\partial w^T_{i}}=(p^{(1)}_i - y^{(1)}_i)x^{(1)}$$

设$$y_i=(y^{(1)}_i,y^{(2)}_i,\dots,y^{(M)}_i),\ p_i=(p^{(1)}_i,p^{(2)}_i,\dots,p^{(M)}_i),\ x_j=(x^{(1)}_j,x^{(2)}_j,\dots,x^{(M)}_j),\ p_i-y_i=(r^{(1)}_i,r^{(2)}_i,\dots,r^{(M)}_i)$$

$$\therefore \frac{\partial L(p)}{\partial w^T_{ij}}=\frac{1}{M}(p_i-y_i)x_j=\frac{1}{M}(r^{(1)}_i,r^{(2)}_i,\dots,r^{(M)}_i)\cdot (x^{(1)}_j,x^{(2)}_j,\dots,x^{(M)}_j)$$（$$\frac{1}{M}$$用来求平均值）

$$\therefore \frac{\partial L(p)}{\partial w^T_{i}}=\frac{1}{M}(p_i-y_i)X^T=\frac{1}{M}(r^{(1)}_i,r^{(2)}_i,\dots,r^{(M)}_i)\begin{pmatrix} -x^{(1)}-\\ -x^{(2)}-\\ \vdots\\ -x^{(M)}- \end{pmatrix}$$

又$$\because P=\begin{pmatrix} p^{(1)}_0&\dots&p^{(M)}_0\\\vdots&\dots&\vdots\\p^{(1)}_9 &\dots&p^{(M)}_9 \end{pmatrix},\ Y=\begin{pmatrix} |&|& &|\\ y^{(1)}&y^{(2)}&\dots&y^{(M)}\\|&|&&| \end{pmatrix}$$

$$\therefore \frac{\partial L(p)}{\partial W}=\frac{1}{M}(P-Y)X^T$$

$$\therefore W:=W-\eta \frac{\partial L(p)}{\partial W}=W-\frac{\eta}{M}(P-Y)X^T$$

**相关资料**

* [Tutorial_HYLee_Deep.pptx (live.com)](https://view.officeapps.live.com/op/view.aspx?src=http%3A%2F%2Fspeech.ee.ntu.edu.tw%2F~tlkagk%2Fslide%2FTutorial_HYLee_Deep.pptx&wdOrigin=BROWSELINK)

* [一文详解Softmax函数](https://zhuanlan.zhihu.com/p/105722023)

* [(强推|双字)2021版吴恩达深度学习课程Deeplearning.ai](https://www.bilibili.com/video/BV12E411a7Xn?p=78)

* [09 Softmax 回归 + 损失函数 + 图片分类数据集【动手学深度学习v2】](https://www.bilibili.com/video/BV1K64y1Q7wu?p=4)

* [详解one-hot编码](https://www.cnblogs.com/shuaishuaidefeizhu/p/11269257.html)

* [反向传播——通俗易懂_](https://blog.csdn.net/weixin_38347387/article/details/82936585?ops_request_misc=%7B%22request%5Fid%22%3A%22164765756916780269893222%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164765756916780269893222&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-82936585.142^v2^es_vector_control_group,143^v4^register&utm_term=反向传播&spm=1018.2226.3001.4187)

* [一文搞懂交叉熵损失](https://www.cnblogs.com/wangguchangqing/p/12068084.html)

* [ numpy快速生成one hot编码](https://blog.csdn.net/sinat_29957455/article/details/86552811?ops_request_misc=&request_id=&biz_id=&utm_medium=distribute.pc_search_result.none-task-blog-2~all~es_rank~default-24-86552811.142^v2^es_vector_control_group,143^v4^register&utm_term=one-hot编码&spm=1018.2226.3001.4187)

* [为什么要做特征归一化/标准化？](https://www.cnblogs.com/shine-lee/p/11779514.html)

* [python中的矩阵运算 ](https://www.cnblogs.com/chamie/p/4870078.html)

* [向量和矩阵的点乘和叉乘](https://blog.csdn.net/wzyaiwl/article/details/106310705?ops_request_misc=%7B%22request%5Fid%22%3A%22164793341716780357240324%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164793341716780357240324&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-106310705.142^v3^pc_search_insert_es_download,143^v4^register&utm_term=矩阵点乘&spm=1018.2226.3001.4187)

